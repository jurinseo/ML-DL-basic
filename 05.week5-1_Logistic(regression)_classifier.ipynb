{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a6f62eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "35d5ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1., 2.], [2., 3.], [3., 1.], [4., 3.], [5., 3.], [6., 2.]]\n",
    "y_train = [[0.],[0.],[0.],[0.],[0.],[1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "806aeebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(len(x_train))\n",
    "\n",
    "W = tf.Variable(tf.zeros([2,1]), name='weight') \n",
    "b = tf.Variable(tf.zeros([1]), name='bias')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "30181c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X):\n",
    "    h = tf.divide(1., 1. + tf.exp(tf.matmul(X, W) + b))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "22b13b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(h, X, Y):\n",
    "    cost = tf.reduce_mean(-Y * tf.math.log(h) - (1 - Y)*tf.math.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2f5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss_function(logistic_regression(X), X, Y)\n",
    "        return tape.gradient(loss_value, [W, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0e8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2501\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for X, Y in iter(dataset):\n",
    "        grads = grad(logistic_regression(X), X, Y)\n",
    "        optimizer.apply_gradients(grads_and_vars = zip(grads, [W, b]))\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(\"Iter: {}, Loss: {:.4f}\".format(epoch, loss_function(logistic_regression(X), X, Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc1b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2ae56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
