{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e72d589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f8eff385",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 3) [[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]] 6\n",
      "(6, 1) [[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]]\n"
     ]
    }
   ],
   "source": [
    "xy = np.loadtxt('data-01-test-score.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "\n",
    "print(x_data.shape, x_data, len(x_data))\n",
    "print(y_data.shape, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e4195cb1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random.normal([3, 1], name='weight'))\n",
    "b = tf.Variable(tf.random.normal([1,], name='bias'))\n",
    "\n",
    "learning_rate = tf.Variable(0.00002)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate, momentum = 0.3)\n",
    "\n",
    "def predict(X):\n",
    "    return tf.matmul(X, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d4ecc7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:     0 |cost:   0.675447 |W1:      1.019 |W2:     0.6341 |W3:     0.3796 | b:   -1.77229\n",
      "epochs:    50 |cost:   0.672968 |W1:       1.02 |W2:     0.6343 |W3:     0.3784 | b:    -1.7724\n",
      "epochs:   100 |cost:   0.670513 |W1:      1.021 |W2:     0.6344 |W3:     0.3773 | b:   -1.77251\n",
      "epochs:   150 |cost:   0.668079 |W1:      1.022 |W2:     0.6346 |W3:     0.3761 | b:   -1.77261\n",
      "epochs:   200 |cost:   0.665655 |W1:      1.023 |W2:     0.6347 |W3:      0.375 | b:   -1.77272\n",
      "epochs:   250 |cost:   0.663261 |W1:      1.024 |W2:     0.6349 |W3:     0.3738 | b:   -1.77283\n",
      "epochs:   300 |cost:   0.660882 |W1:      1.025 |W2:      0.635 |W3:     0.3727 | b:   -1.77294\n",
      "epochs:   350 |cost:   0.658518 |W1:      1.026 |W2:     0.6352 |W3:     0.3715 | b:   -1.77304\n",
      "epochs:   400 |cost:   0.656177 |W1:      1.027 |W2:     0.6353 |W3:     0.3704 | b:   -1.77315\n",
      "epochs:   450 |cost:   0.653847 |W1:      1.028 |W2:     0.6354 |W3:     0.3693 | b:   -1.77326\n",
      "epochs:   500 |cost:   0.651545 |W1:      1.029 |W2:     0.6356 |W3:     0.3681 | b:   -1.77337\n",
      "epochs:   550 |cost:   0.649247 |W1:       1.03 |W2:     0.6357 |W3:      0.367 | b:   -1.77347\n",
      "epochs:   600 |cost:   0.646967 |W1:      1.031 |W2:     0.6359 |W3:     0.3659 | b:   -1.77358\n",
      "epochs:   650 |cost:    0.64472 |W1:      1.032 |W2:      0.636 |W3:     0.3648 | b:   -1.77369\n",
      "epochs:   700 |cost:   0.642481 |W1:      1.033 |W2:     0.6362 |W3:     0.3636 | b:   -1.77379\n",
      "epochs:   750 |cost:   0.640254 |W1:      1.034 |W2:     0.6364 |W3:     0.3625 | b:    -1.7739\n",
      "epochs:   800 |cost:   0.638051 |W1:      1.035 |W2:     0.6365 |W3:     0.3614 | b:   -1.77401\n",
      "epochs:   850 |cost:   0.635863 |W1:      1.036 |W2:     0.6367 |W3:     0.3603 | b:   -1.77412\n",
      "epochs:   900 |cost:   0.633682 |W1:      1.037 |W2:     0.6368 |W3:     0.3592 | b:   -1.77422\n",
      "epochs:   950 |cost:   0.631531 |W1:      1.038 |W2:      0.637 |W3:     0.3581 | b:   -1.77433\n",
      "epochs:  1000 |cost:   0.629387 |W1:      1.039 |W2:     0.6371 |W3:      0.357 | b:   -1.77444\n",
      "epochs:  1050 |cost:   0.627264 |W1:       1.04 |W2:     0.6373 |W3:      0.356 | b:   -1.77455\n",
      "epochs:  1100 |cost:    0.62516 |W1:      1.041 |W2:     0.6374 |W3:     0.3549 | b:   -1.77465\n",
      "epochs:  1150 |cost:   0.623079 |W1:      1.042 |W2:     0.6376 |W3:     0.3538 | b:   -1.77476\n",
      "epochs:  1200 |cost:   0.621007 |W1:      1.043 |W2:     0.6377 |W3:     0.3527 | b:   -1.77487\n",
      "epochs:  1250 |cost:   0.618946 |W1:      1.044 |W2:     0.6379 |W3:     0.3516 | b:   -1.77497\n",
      "epochs:  1300 |cost:   0.616899 |W1:      1.045 |W2:     0.6381 |W3:     0.3506 | b:   -1.77508\n",
      "epochs:  1350 |cost:   0.614874 |W1:      1.046 |W2:     0.6382 |W3:     0.3495 | b:   -1.77519\n",
      "epochs:  1400 |cost:   0.612857 |W1:      1.047 |W2:     0.6384 |W3:     0.3484 | b:    -1.7753\n",
      "epochs:  1450 |cost:   0.610863 |W1:      1.048 |W2:     0.6385 |W3:     0.3474 | b:    -1.7754\n",
      "epochs:  1500 |cost:   0.608878 |W1:      1.049 |W2:     0.6387 |W3:     0.3463 | b:   -1.77551\n",
      "epochs:  1550 |cost:   0.606911 |W1:      1.049 |W2:     0.6388 |W3:     0.3453 | b:   -1.77562\n",
      "epochs:  1600 |cost:   0.604956 |W1:       1.05 |W2:      0.639 |W3:     0.3442 | b:   -1.77573\n",
      "epochs:  1650 |cost:    0.60302 |W1:      1.051 |W2:     0.6391 |W3:     0.3432 | b:   -1.77583\n",
      "epochs:  1700 |cost:   0.601102 |W1:      1.052 |W2:     0.6393 |W3:     0.3422 | b:   -1.77594\n",
      "epochs:  1750 |cost:   0.599193 |W1:      1.053 |W2:     0.6394 |W3:     0.3411 | b:   -1.77605\n",
      "epochs:  1800 |cost:   0.597301 |W1:      1.054 |W2:     0.6396 |W3:     0.3401 | b:   -1.77615\n",
      "epochs:  1850 |cost:   0.595413 |W1:      1.055 |W2:     0.6398 |W3:     0.3391 | b:   -1.77626\n",
      "epochs:  1900 |cost:   0.593549 |W1:      1.056 |W2:     0.6399 |W3:      0.338 | b:   -1.77637\n",
      "epochs:  1950 |cost:   0.591697 |W1:      1.057 |W2:     0.6401 |W3:      0.337 | b:   -1.77648\n",
      "epochs:  2000 |cost:   0.589866 |W1:      1.057 |W2:     0.6402 |W3:      0.336 | b:   -1.77658\n",
      "epochs:  2050 |cost:   0.588044 |W1:      1.058 |W2:     0.6404 |W3:      0.335 | b:   -1.77669\n",
      "epochs:  2100 |cost:   0.586226 |W1:      1.059 |W2:     0.6405 |W3:      0.334 | b:    -1.7768\n",
      "epochs:  2150 |cost:   0.584442 |W1:       1.06 |W2:     0.6407 |W3:      0.333 | b:   -1.77691\n",
      "epochs:  2200 |cost:   0.582663 |W1:      1.061 |W2:     0.6408 |W3:      0.332 | b:   -1.77701\n",
      "epochs:  2250 |cost:   0.580886 |W1:      1.062 |W2:      0.641 |W3:      0.331 | b:   -1.77711\n",
      "epochs:  2300 |cost:   0.579141 |W1:      1.063 |W2:     0.6412 |W3:       0.33 | b:   -1.77722\n",
      "epochs:  2350 |cost:   0.577403 |W1:      1.064 |W2:     0.6413 |W3:      0.329 | b:   -1.77732\n",
      "epochs:  2400 |cost:   0.575668 |W1:      1.064 |W2:     0.6415 |W3:      0.328 | b:   -1.77742\n",
      "epochs:  2450 |cost:   0.573951 |W1:      1.065 |W2:     0.6416 |W3:      0.327 | b:   -1.77752\n"
     ]
    }
   ],
   "source": [
    "for i in range(2500):\n",
    "    with tf.GradientTape(persistent=False) as tape:\n",
    "        hypothesis = tf.matmul(x_data, W) + b\n",
    "        cost = tf.reduce_mean(tf.square(hypothesis - y_data))\n",
    "        W_grad, b_grad= tape.gradient(cost, [W, b])\n",
    "#         optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "        W.assign_sub(learning_rate * W_grad)\n",
    "        b.assign_sub(learning_rate * b_grad)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print(\"epochs: {:5} |cost: {:10.6} |W1: {:10.4} |W2: {:10.4} |W3: {:10.4} | b: {:10.6}\"\n",
    "              .format(i, cost, W.numpy()[0][0], W.numpy()[1][0], W.numpy()[2][0], b.numpy()[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5a1eee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0660331 ]\n",
      " [0.6417609 ]\n",
      " [0.32607526]]\n",
      "[[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]\n",
      " [ 53.  46.  55.]]\n",
      "[[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]\n",
      " [101.]]\n"
     ]
    }
   ],
   "source": [
    "print(W.numpy())\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b82dbd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[151.83931],\n",
       "       [184.16342],\n",
       "       [180.84634],\n",
       "       [196.06166],\n",
       "       [141.22429],\n",
       "       [102.17728]], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(x_data).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d9c4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[184.06555 ],\n",
       "       [ 60.126774]], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([[89., 95., 92], [11, 35, 85]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ec0ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "py:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
